{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMtooneAS8w/g6PXiCX4yPh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"m6bP3NIW0wPf"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sqlalchemy import create_engine, text\n","\n","PG_USER = \"neondb_owner\"\n","PG_PASS = \"npg_TvRM6PWJrkD2\"\n","PG_HOST = \"ep-muddy-lab-agi3j2y5-pooler.c-2.eu-central-1.aws.neon.tech\"\n","PG_PORT = \"5432\"\n","PG_DB   = \"neondb\"\n","\n","# connect to Neon\n","engine = create_engine(\n","    f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",")\n","\n","def _safe_div(numer, denom):\n","    \"\"\"Vectorized division that avoids divide-by-zero errors.\"\"\"\n","    return np.where(denom == 0, np.nan, numer / denom)\n","\n","def run_pipeline(new_batch_df: pd.DataFrame):\n","    \"\"\"\n","    1. Append new batch into stg_campaigns_raw\n","    2. Read full staging\n","    3. Clean + dedupe\n","    4. Feature engineering\n","    5. Refresh fact_campaigns_clean\n","    \"\"\"\n","\n","    # 1. append batch to staging in Neon and read full staging\n","    with engine.begin() as conn:\n","        # get actual column order from stg_campaigns_raw in DB\n","        table_info_query = \"\"\"\n","            SELECT column_name\n","            FROM information_schema.columns\n","            WHERE table_name = 'stg_campaigns_raw'\n","            ORDER BY ordinal_position;\n","        \"\"\"\n","        existing_cols = pd.read_sql(text(table_info_query), conn)[\"column_name\"].tolist()\n","\n","        # align incoming batch to staging schema\n","        batch_cols_to_use = [c for c in new_batch_df.columns if c in existing_cols]\n","        df_batch_aligned = new_batch_df[batch_cols_to_use].copy()\n","\n","        # append into staging\n","        df_batch_aligned.to_sql(\n","            \"stg_campaigns_raw\",\n","            con=conn,\n","            if_exists=\"replace\",\n","            index=False\n","        )\n","\n","        # read full staging after append\n","        df_raw = pd.read_sql(text(\"SELECT * FROM stg_campaigns_raw;\"), conn)\n","\n","    # 2. cleaning\n","\n","    # drop duplicate \".1\" columns that are identical to the base column\n","    cols_to_drop = []\n","    for col in list(df_raw.columns):\n","        if col.endswith(\".1\"):\n","            base = col[:-2]\n","            if base in df_raw.columns and df_raw[base].equals(df_raw[col]):\n","                cols_to_drop.append(col)\n","    if cols_to_drop:\n","        df_raw = df_raw.drop(columns=cols_to_drop)\n","\n","    # drop exact duplicate rows\n","    df_clean = df_raw.drop_duplicates()\n","\n","    # drop rows missing critical business fields\n","    required_cols = [\"c_date\", \"campaign_name\", \"impressions\", \"clicks\", \"mark_spent\", \"revenue\"]\n","    existing_required = [c for c in required_cols if c in df_clean.columns]\n","    df_clean = df_clean.dropna(subset=existing_required)\n","\n","    # impressions must be > 0\n","    if \"impressions\" in df_clean.columns:\n","        df_clean = df_clean[df_clean[\"impressions\"] > 0]\n","\n","    # numeric columns must be >= 0\n","    numeric_cols = [\"impressions\", \"clicks\", \"leads\", \"orders\", \"mark_spent\", \"revenue\"]\n","    for col in numeric_cols:\n","        if col in df_clean.columns:\n","            df_clean = df_clean[df_clean[col] >= 0]\n","\n","    # normalize date\n","    if \"c_date\" in df_clean.columns:\n","        df_clean[\"c_date\"] = pd.to_datetime(df_clean[\"c_date\"], errors=\"coerce\")\n","        df_clean = df_clean.dropna(subset=[\"c_date\"])\n","        df_clean[\"c_date\"] = df_clean[\"c_date\"].dt.strftime(\"%Y-%m-%d\")\n","\n","    # dedupe by id (keep latest by date)\n","    if \"id\" in df_clean.columns:\n","        tmp = df_clean.copy()\n","        tmp[\"__c_date_dt\"] = pd.to_datetime(tmp[\"c_date\"], errors=\"coerce\")\n","        tmp = tmp.sort_values(\"__c_date_dt\")\n","        df_clean = tmp.drop_duplicates(subset=[\"id\"], keep=\"last\").drop(columns=\"__c_date_dt\")\n","\n","    # 3. feature engineering\n","    df_feat = df_clean.copy()\n","    dt_tmp = pd.to_datetime(df_feat[\"c_date\"], errors=\"coerce\")\n","\n","    # marketing KPIs\n","    df_feat[\"CTR_pct\"] = _safe_div(df_feat.get(\"clicks\", np.nan), df_feat.get(\"impressions\", np.nan)) * 100\n","    df_feat[\"CPC\"] = _safe_div(df_feat.get(\"mark_spent\", np.nan), df_feat.get(\"clicks\", np.nan))\n","    df_feat[\"CPA\"] = _safe_div(df_feat.get(\"mark_spent\", np.nan), df_feat.get(\"orders\", np.nan))\n","    df_feat[\"ConversionRate_pct\"] = _safe_div(df_feat.get(\"orders\", np.nan), df_feat.get(\"clicks\", np.nan)) * 100\n","    df_feat[\"ROAS\"] = _safe_div(df_feat.get(\"revenue\", np.nan), df_feat.get(\"mark_spent\", np.nan))\n","    df_feat[\"Profit\"] = df_feat.get(\"revenue\", np.nan) - df_feat.get(\"mark_spent\", np.nan)\n","    df_feat[\"LeadRate_pct\"] = _safe_div(df_feat.get(\"leads\", np.nan), df_feat.get(\"clicks\", np.nan)) * 100\n","\n","    # time features\n","    df_feat[\"Year\"] = dt_tmp.dt.year\n","    df_feat[\"Month\"] = dt_tmp.dt.month\n","    df_feat[\"Weekday\"] = dt_tmp.dt.day_name()\n","    df_feat[\"Is_Weekend\"] = dt_tmp.dt.weekday.isin([5, 6]).astype(int)\n","\n","    # round readable KPIs\n","    round_cols = [\"CTR_pct\", \"ConversionRate_pct\", \"LeadRate_pct\", \"CPC\", \"CPA\", \"ROAS\", \"Profit\"]\n","    for c in round_cols:\n","        if c in df_feat.columns:\n","            df_feat[c] = df_feat[c].round(2)\n","\n","    # 4. rename columns to match Postgres table column names (all lowercase snake_case)\n","    rename_map = {\n","        \"CTR_pct\": \"ctr_pct\",\n","        \"CPC\": \"cpc\",\n","        \"CPA\": \"cpa\",\n","        \"ConversionRate_pct\": \"conversionrate_pct\",\n","        \"ROAS\": \"roas\",\n","        \"Profit\": \"profit\",\n","        \"LeadRate_pct\": \"leadrate_pct\",\n","        \"Year\": \"year\",\n","        \"Month\": \"month\",\n","        \"Weekday\": \"weekday\",\n","        \"Is_Weekend\": \"is_weekend\",\n","        # the base columns like id, c_date, campaign_name, category, campaign_id,\n","        # impressions, mark_spent, clicks, leads, orders, revenue\n","        # are already lowercase, so we don't touch them\n","    }\n","    df_out = df_feat.rename(columns=rename_map)\n","\n","    # 5. write snapshot to fact_campaigns_clean\n","    with engine.begin() as conn:\n","        # clear old snapshot but keep table structure\n","        conn.execute(text(\"TRUNCATE TABLE fact_campaigns_clean;\"))\n","\n","        # insert new snapshot\n","        df_out.to_sql(\n","            \"fact_campaigns_clean\",\n","            con=conn,\n","            if_exists=\"append\",\n","            index=False\n","        )\n","\n","    print(\"âœ… Pipeline finished successfully.\")\n","    print(f\" - Rows in staging: {len(df_raw)}\")\n","    print(f\" - Rows in fact_campaigns_clean: {len(df_out)}\")\n","    print(\" - Columns in fact_campaigns_clean:\")\n","    print(df_out.columns.tolist())\n","\n","    return df_out.head()"]}]}